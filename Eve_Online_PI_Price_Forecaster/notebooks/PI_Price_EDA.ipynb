{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import hashlib\n",
    "from os import path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from datetime import datetime\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "\"\"\"---------------------- env constants --------------------------\"\"\"\n",
    "\n",
    "# find .env by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\"\"\"---------------------- database constants --------------------------\"\"\"\n",
    "\n",
    "DATABASE_URL = os.environ.get('DATABASE_URL')\n",
    "DATABASE_NAME = os.environ.get(\"DATABASE_NAME\")\n",
    "DATABASE_USER = os.environ.get(\"DATABASE_USER\")\n",
    "DATABASE_PASSWORD = os.environ.get(\"DATABASE_PASSWORD\")\n",
    "DATABASE_PORT = os.environ.get(\"DATABASE_PORT\")\n",
    "\n",
    "\"\"\"---------------------- filepath and id constants --------------------------\"\"\"\n",
    "\n",
    "# Filepath that contains the items that we are interested in forcasting\n",
    "ABS_FILE_PATH_ITEMS = path.abspath('../src/data/items.txt')\n",
    "\n",
    "# region ids that will be used in the function collect_data \n",
    "REGIONS = [0, 30000142, 30000144, 60003760, 60008494, 60011866, 60004588, 60005686]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tables():\n",
    "    \"\"\" create tables in the PostgreSQL database\"\"\"\n",
    "    command = (\n",
    "        \"\"\"\n",
    "        CREATE TABLE market_data (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            time TIMESTAMP,\n",
    "            region_id INTEGER,\n",
    "            data JSON\n",
    "        )\n",
    "        \"\"\")\n",
    "    \n",
    "    conn = psycopg2.connect(host=DATABASE_URL,\n",
    "                            database=DATABASE_NAME,\n",
    "                            user=DATABASE_USER,\n",
    "                            password=DATABASE_PASSWORD,\n",
    "                            port=DATABASE_PORT)\n",
    "    try:\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # create table one by one\n",
    "        cur.execute(command)\n",
    "        # close communication with the PostgreSQL database server\n",
    "        cur.close()\n",
    "        # commit the changes\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Table created successfully\")\n",
    "        \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully\n"
     ]
    }
   ],
   "source": [
    "#create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Data Collection](#data-collection)\n",
    "2. [Data Validation](#data-validation)\n",
    "3. [Preprocessing](#preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hashs(hash_for_items_path='../data/interim/hash_for_items.p'):\n",
    "    \"\"\" Compares the current hash to the hash of items.txt and returns True when the same otherwise\n",
    "    returns false.\n",
    "\n",
    "    Args:\n",
    "        hash_for_items_path (file path): The file path for the current hash we want to compare.\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def check_if_items_changed():\n",
    "    \"\"\" Check for item added/removed from src/data/items.txt. If changes are detected a new hash will be created and items.p will be stored in data/interim for future usage to reduce API calls\n",
    "    for getting typeIDs for raw material names. If pickle_items.p doesn't exist returns True by default.\n",
    "\n",
    "    Returns:\n",
    "        Boolean: if file has changed than returns True otherwise returns False. Default to True if file non existant.\n",
    "    \"\"\"\n",
    "    \n",
    "    # file path for pickled_items and items_hash if they exist\n",
    "    pickled_items_path = path.abspath('../data/interim/pickle_items.p')\n",
    "    items_hash_path = path.abspath('../data/interim/hash_for_items.p')\n",
    "    \n",
    "    # Boolean variables to determine if those files exist\n",
    "    pickled_items_exists = os.path.exists(pickled_items_path)\n",
    "    items_hash_exists = os.path.exists(items_hash_path)\n",
    "    \n",
    "    # If the files doesn't exist will Return\n",
    "    if not pickled_items_exists:\n",
    "        print(\"Did not find \", pickled_items_path)\n",
    "        return True\n",
    "    else:\n",
    "        if not items_hash_exists:\n",
    "            print(\"Did not find \", items_hash_path)\n",
    "            return True\n",
    "        else:\n",
    "            hexadecimal_match = compare_hashs()\n",
    "    \n",
    "    return True\n",
    "    \n",
    "\n",
    "def get_raw_material_names():\n",
    "    \"\"\" Return a list of raw material names from from ABS_FILE_PATH_ITEMS\n",
    "\n",
    "    Returns:\n",
    "        raw_material_names: item names of interest for forcast\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # List that will be returned after it has been populated\n",
    "    raw_material_names = []\n",
    "    \n",
    "    if path.exists(ABS_FILE_PATH_ITEMS):\n",
    "        \n",
    "        file = open(ABS_FILE_PATH_ITEMS, \"r\")\n",
    "        \n",
    "        # Read each line from ABS_FILE_PATH_ITEMS and append each item name into the list\n",
    "        for item in file.readlines():\n",
    "            raw_material_names.append(item.strip('\\n'))\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "        # Make sure that the item names are not repeated\n",
    "        raw_material_names = list(set(raw_material_names))\n",
    "    \n",
    "        return raw_material_names\n",
    "        \n",
    "    else:\n",
    "        print(ABS_FILE_PATH_ITEMS, \"does not exist.\")\n",
    "\n",
    "def get_item_id(item_name=None):\n",
    "    \"\"\" Returns the item ID from using the API endpoint https://www.fuzzwork.co.uk/api/typeid.php?typename=Silicon\n",
    "\n",
    "    Args:\n",
    "        item_name (string): Name of the raw material. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        int: ids that are of type int\n",
    "    \"\"\"\n",
    "    \n",
    "    item_id = None\n",
    "    \n",
    "    api_url = \"https://www.fuzzwork.co.uk/api/typeid.php?typename=\" + item_name\n",
    "    r = requests.get(api_url)\n",
    "    item_id = r.json()['typeID']\n",
    "    \n",
    "    if item_id:\n",
    "        return item_id\n",
    "    \n",
    "def fetch_data(region_id, item_id):\n",
    "    \"\"\" Returns JSON given an input of the region and item from an API\n",
    "\n",
    "    Args:\n",
    "        region_id (int): id that is assigned to each major market region.\n",
    "        item_id (int, list): id or list of ids that is/are assigned to each raw material.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON Data from https://market.fuzzwork.co.uk/aggregates/?region=30000142&types=9828\n",
    "    \"\"\"\n",
    "    \n",
    "    # Formating so that it'll be accepted by the API endpoint\n",
    "    item_id = str(item_id).strip('[]').replace(\" \", \"\")\n",
    "    \n",
    "    api_url = \"https://market.fuzzwork.co.uk/aggregates/?region=\" + str(region_id) + \"&types=\" + item_id\n",
    "    r = requests.get(api_url)\n",
    "    \n",
    "    # encoding as json\n",
    "    raw_material_data = r.json()\n",
    "    \n",
    "    return raw_material_data\n",
    "\n",
    "def insert_data(table_name='market_data', region_id=None, json_data=None):\n",
    "    \"\"\" Connects with postgresql database and inserts records into a table.\n",
    "\n",
    "    Args:\n",
    "        table_name (str, optional): _description_. Defaults to 'raw_material_data'.\n",
    "        region_id (int, optional): _description_. Defaults to None.\n",
    "        json_data (json, optional): _description_. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(host=DATABASE_URL,\n",
    "                                database=DATABASE_NAME,\n",
    "                                user=DATABASE_USER,\n",
    "                                password=DATABASE_PASSWORD)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # sql to insert the json into table\n",
    "        insert_command =\"INSERT INTO \" + table_name + \" (TIME, REGION_ID, DATA) VALUES (%s, %s, %s)\"\n",
    "        \n",
    "        # Serializing json\n",
    "        json_object = json.dumps(json_data)\n",
    "        \n",
    "        # Inserting into table\n",
    "        values = (time, region_id, json_object)\n",
    "        cursor.execute(insert_command, values)\n",
    "        \n",
    "        # Close and commit changes to database server\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Successfully stored data into table for region ID: \" + str(region_id) + \".\")\n",
    "        \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement concurrency when doing API calls\n",
    "\n",
    "def store_data():\n",
    "    \"\"\" GET Requests the Eve Online API endpoint https://market.fuzzwork.co.uk/aggregates/?region=30000142&types=9828 \n",
    "    which takes two params region and types. This JSON data is then stored\n",
    "    \n",
    "    Region: The location that the markets are located in\n",
    "    - There are 7 Regions\n",
    "        - Global - 0\n",
    "        - Jita - 30000142\n",
    "        - Perimeter - 30000144\n",
    "        - Jita 4-4 CNAP - 60003760\n",
    "        - Amarr VIII - 60008494\n",
    "        - Dodixie - 60011866\n",
    "        - Rens - 60004588\n",
    "        - Hek - 60005686\n",
    "        \n",
    "    Types: The ID of the raw material, List of IDs: https://docs.google.com/spreadsheets/d/1X7mi7j-_yV5lq-Yd2BraE-t4QE_a4IKv2ZuCBSLD6QU/edit?usp=sharing\n",
    "    \"\"\"\n",
    "    \n",
    "    items = get_raw_material_names()\n",
    "    item_ids = [ get_item_id(i) for i in items ]\n",
    "\n",
    "    json_data = {}\n",
    "\n",
    "    # Fetch the data for each region and raw material id\n",
    "    print(\"Attempting to fetch and insert data...\")\n",
    "    with alive_bar(len(REGIONS), force_tty=True) as bar:\n",
    "        for r in REGIONS:\n",
    "            \n",
    "            # Fetching the data\n",
    "            data = fetch_data(region_id=r, item_id=item_ids)\n",
    "            \n",
    "            # Storing the data into a postgresql table\n",
    "            insert_data(table_name='market_data', region_id=r, json_data=data)\n",
    "            bar()\n",
    "    \n",
    "    print(\"Successfully stored data for all regions!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_raw_material_names()\n",
    "item_ids = [ get_item_id(i) for i in items ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the list of item ids so we don't have to make api calls again if items.txt hasn't changed\n",
    "with open('../data/interim/pickle_items.p', 'wb') as pickle_file:\n",
    "    pickle_items = pickle.dump(item_ids, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2348,\n",
       " 9842,\n",
       " 2307,\n",
       " 3725,\n",
       " 9830,\n",
       " 2366,\n",
       " 12836,\n",
       " 2328,\n",
       " 2305,\n",
       " 9828,\n",
       " 2321,\n",
       " 2360,\n",
       " 2397,\n",
       " 2389,\n",
       " 3697,\n",
       " 2352,\n",
       " 2344,\n",
       " 2327,\n",
       " 2868,\n",
       " 2367,\n",
       " 2329,\n",
       " 17392,\n",
       " 2309,\n",
       " 2270,\n",
       " 2308,\n",
       " 2319,\n",
       " 17136,\n",
       " 2272,\n",
       " 2310,\n",
       " 2872,\n",
       " 2287,\n",
       " 2876,\n",
       " 44,\n",
       " 9840,\n",
       " 3645,\n",
       " 2288,\n",
       " 9832,\n",
       " 3683,\n",
       " 2867,\n",
       " 9846,\n",
       " 2390,\n",
       " 2400,\n",
       " 2267,\n",
       " 2869,\n",
       " 2392,\n",
       " 2399,\n",
       " 3779,\n",
       " 17898,\n",
       " 2401,\n",
       " 2346,\n",
       " 9834,\n",
       " 2870,\n",
       " 2345,\n",
       " 9838,\n",
       " 3695,\n",
       " 9836,\n",
       " 2311,\n",
       " 3775,\n",
       " 9848,\n",
       " 28974,\n",
       " 2351,\n",
       " 2268,\n",
       " 2393,\n",
       " 3693,\n",
       " 2398,\n",
       " 2354,\n",
       " 2286,\n",
       " 2875,\n",
       " 15317,\n",
       " 2395,\n",
       " 2463,\n",
       " 2871,\n",
       " 2306,\n",
       " 2396,\n",
       " 3828,\n",
       " 3691,\n",
       " 2358,\n",
       " 2361,\n",
       " 2312,\n",
       " 2349,\n",
       " 3689,\n",
       " 2073,\n",
       " 2317]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/interim/pickle_items.p', 'rb') as pickle_file:\n",
    "    item_ids = pickle.load(pickle_file)\n",
    "    \n",
    "item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/data/items.txt does not exist.\n",
      "Attempting to fetch and insert data...\n",
      "on 0: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 1: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 2: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 3: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 4: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 5: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 6: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 7: can only concatenate str (not \"int\") to str                                                                       \n",
      "|████████████████████████████████████████| 8/8 [100%] in 3.8s (2.10/s)                                                  \n",
      "Successfully stored data for all regions!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    store_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c7c545e18f71168b0f0faa889157618686ac54bcfd377c91147b26dbd4ffa70"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
