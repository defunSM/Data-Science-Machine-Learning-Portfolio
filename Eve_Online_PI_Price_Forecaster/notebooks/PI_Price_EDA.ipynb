{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import hashlib\n",
    "from os import path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from datetime import datetime\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.data.hash import compute_hash\n",
    "from src.data.constants import ABS_FILE_PATH_ITEMS\n",
    "from src.data.database import fetch_and_store_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'460ac842e32a71a7b96a80dfbbb5b3413ef6ea39df746ada285fbbc2fcb77f94'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_hash(ABS_FILE_PATH_ITEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch and insert data...\n",
      "on 0: Successfully stored data into table for region ID: 0.                                                             \n",
      "on 1: Successfully stored data into table for region ID: 30000142.                                                      \n",
      "on 2: Successfully stored data into table for region ID: 30000144.                                                      \n",
      "on 3: Successfully stored data into table for region ID: 60003760.                                                      \n",
      "on 4: Successfully stored data into table for region ID: 60008494.                                                      \n",
      "on 5: Successfully stored data into table for region ID: 60011866.                                                      \n",
      "on 6: Successfully stored data into table for region ID: 60004588.                                                      \n",
      "on 7: Successfully stored data into table for region ID: 60005686.                                                      \n",
      "|████████████████████████████████████████| 8/8 [100%] in 4.5s (1.78/s)                                                  \n",
      "Successfully stored data for all regions!\n"
     ]
    }
   ],
   "source": [
    "fetch_and_store_data('test_market_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_database():\n",
    "    \"\"\" Connects to the postgresql database\n",
    "\n",
    "    Returns:\n",
    "        database connection: used to execute sql queries or creating tables.\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = psycopg2.connect(host=DATABASE_URL,\n",
    "                            database=DATABASE_NAME,\n",
    "                            user=DATABASE_USER,\n",
    "                            password=DATABASE_PASSWORD,\n",
    "                            port=DATABASE_PORT)\n",
    "    return conn\n",
    "\n",
    "def create_table(table_name):\n",
    "    \"\"\" create a table by the name of 'table_name' provided in the PostgreSQL database\"\"\"\n",
    "    command = (\n",
    "        \"\"\"\n",
    "        CREATE TABLE \"\"\" + table_name + \"\"\" (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            time TIMESTAMP,\n",
    "            region_id INTEGER,\n",
    "            data JSON\n",
    "        )\n",
    "        \"\"\")\n",
    "    \n",
    "    conn = connect_to_database()\n",
    "    try:\n",
    "\n",
    "        # connect to the PostgreSQL server\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # create table one by one\n",
    "        cur.execute(command)\n",
    "        # close communication with the PostgreSQL database server\n",
    "        cur.close()\n",
    "        # commit the changes\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"Table\", table_name, \"created successfully\")\n",
    "        \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Data Collection](#data-collection)\n",
    "2. [Data Validation](#data-validation)\n",
    "3. [Preprocessing](#preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(filepath):\n",
    "    \"\"\" Hashlib library to compute a hash for a file using sha256 algo.\n",
    "\n",
    "    Args:\n",
    "        filepath (string): path to the file in which we want to hash\n",
    "\n",
    "    Returns:\n",
    "        string: A hexadecimal string of the hash\n",
    "    \"\"\"\n",
    "    \n",
    "    # The size of each read from the file (64Kb)\n",
    "    BLOCK_SIZE = 65536 \n",
    "\n",
    "    # Create the hash object\n",
    "    file_hash = hashlib.sha256() \n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        fb = f.read(BLOCK_SIZE)\n",
    "        \n",
    "        # While there is still data being read from the file\n",
    "        while len(fb) > 0: \n",
    "            file_hash.update(fb) \n",
    "            fb = f.read(BLOCK_SIZE) \n",
    "\n",
    "    return file_hash.hexdigest() # Get the hexadecimal digest of the hash\n",
    "    \n",
    "\n",
    "def get_raw_material_names():\n",
    "    \"\"\" Return a list of raw material names from from ABS_FILE_PATH_ITEMS\n",
    "\n",
    "    Returns:\n",
    "        raw_material_names: item names of interest for forcast\n",
    "    \"\"\"\n",
    "    \n",
    "    # List that will be returned after it has been populated\n",
    "    raw_material_names = []\n",
    "    \n",
    "    if path.exists(ABS_FILE_PATH_ITEMS):\n",
    "\n",
    "        file = open(ABS_FILE_PATH_ITEMS, \"r\")\n",
    "        \n",
    "        # Read each line from ABS_FILE_PATH_ITEMS and append each item name into the list\n",
    "        for item in file.readlines():\n",
    "            raw_material_names.append(item.strip('\\n'))\n",
    "\n",
    "        file.close()\n",
    "        \n",
    "        # Make sure that the item names are not repeated\n",
    "        raw_material_names = list(set(raw_material_names))\n",
    "        \n",
    "        print(raw_material_names)\n",
    "        return raw_material_names\n",
    "        \n",
    "    else:\n",
    "        print(ABS_FILE_PATH_ITEMS, \"does not exist.\")\n",
    "\n",
    "def get_item_id(item_name=None):\n",
    "    \"\"\" Returns the item ID from using the API endpoint https://www.fuzzwork.co.uk/api/typeid.php?typename=Silicon\n",
    "\n",
    "    Args:\n",
    "        item_name (string): Name of the raw material. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        int: ids that are of type int\n",
    "    \"\"\"\n",
    "    \n",
    "    item_id = None\n",
    "    \n",
    "    api_url = \"https://www.fuzzwork.co.uk/api/typeid.php?typename=\" + item_name\n",
    "    r = requests.get(api_url)\n",
    "    item_id = r.json()['typeID']\n",
    "    \n",
    "    if item_id:\n",
    "        return item_id\n",
    "    \n",
    "def fetch_data(region_id, item_id):\n",
    "    \"\"\" Returns JSON given an input of the region and item from an API\n",
    "\n",
    "    Args:\n",
    "        region_id (int): id that is assigned to each major market region.\n",
    "        item_id (int, list): id or list of ids that is/are assigned to each raw material.\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON Data from https://market.fuzzwork.co.uk/aggregates/?region=30000142&types=9828\n",
    "    \"\"\"\n",
    "    \n",
    "    # Formating so that it'll be accepted by the API endpoint\n",
    "    item_id = str(item_id).strip('[]').replace(\" \", \"\")\n",
    "    \n",
    "    api_url = \"https://market.fuzzwork.co.uk/aggregates/?region=\" + str(region_id) + \"&types=\" + item_id\n",
    "    r = requests.get(api_url)\n",
    "    \n",
    "    # encoding as json\n",
    "    raw_material_data = r.json()\n",
    "    \n",
    "    return raw_material_data\n",
    "\n",
    "def insert_data(table_name='market_data', region_id=None, json_data=None):\n",
    "    \"\"\" Connects with postgresql database and inserts records into a table.\n",
    "\n",
    "    Args:\n",
    "        table_name (str, optional): _description_. Defaults to 'raw_material_data'.\n",
    "        region_id (int, optional): _description_. Defaults to None.\n",
    "        json_data (json, optional): _description_. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        conn = connect_to_database()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # sql to insert the json into table\n",
    "        insert_command =\"INSERT INTO \" + table_name + \" (TIME, REGION_ID, DATA) VALUES (%s, %s, %s)\"\n",
    "        \n",
    "        # Serializing json\n",
    "        json_object = json.dumps(json_data)\n",
    "        \n",
    "        # Inserting into table\n",
    "        values = (time, region_id, json_object)\n",
    "        cursor.execute(insert_command, values)\n",
    "        \n",
    "        # Close and commit changes to database server\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"Successfully stored data into table for region ID: \" + str(region_id) + \".\")\n",
    "        \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions related to serialization\n",
    "\n",
    "def pickle_data(item, filepath):\n",
    "    \"\"\" Picklizes the 'item' and saves it to a 'filepath'.\n",
    "\n",
    "    Args:\n",
    "        item (any): Object to be picklized\n",
    "        filepath (string): The filepath that you want to save the picklized object to.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'wb') as pickle_file:\n",
    "        pickle_items = pickle.dump(item, pickle_file)\n",
    "        \n",
    "def load_pickle_data(filepath):\n",
    "    \"\"\" Loads a picklized object from a pickle 'filepath'.\n",
    "\n",
    "    Args:\n",
    "        filepath (string): filepath to the picklized data\n",
    "\n",
    "    Returns:\n",
    "        any: The picklized data that was stored in the filepath\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filepath, 'rb') as pickle_file:\n",
    "        data = pickle.load(pickle_file)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement concurrency when doing API calls\n",
    "\n",
    "def fetch_and_store_data(table_name=\"market_data\"):\n",
    "    \"\"\" GET Requests the Eve Online API endpoint https://market.fuzzwork.co.uk/aggregates/?region=30000142&types=9828 \n",
    "    which takes two params region and types. This JSON data is then stored\n",
    "    \n",
    "    Region: The location that the markets are located in\n",
    "    - There are 7 Regions\n",
    "        - Global - 0\n",
    "        - Jita - 30000142\n",
    "        - Perimeter - 30000144\n",
    "        - Jita 4-4 CNAP - 60003760\n",
    "        - Amarr VIII - 60008494\n",
    "        - Dodixie - 60011866\n",
    "        - Rens - 60004588\n",
    "        - Hek - 60005686\n",
    "        \n",
    "    Types: The ID of the raw material, List of IDs: https://docs.google.com/spreadsheets/d/1X7mi7j-_yV5lq-Yd2BraE-t4QE_a4IKv2ZuCBSLD6QU/edit?usp=sharing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checks if items in items.txt has changed and if it did makes api calls\n",
    "    old_hash = load_pickle_data(HASH_PATH)\n",
    "    new_hash = compute_hash(ABS_FILE_PATH_ITEMS)\n",
    "    \n",
    "    if old_hash != new_hash:\n",
    "        items = get_raw_material_names()\n",
    "        item_ids = [ get_item_id(i) for i in items ]\n",
    "        pickle_data(item_ids, HASH_PATH)\n",
    "        \n",
    "    else:\n",
    "        item_ids = load_pickle_data(ITEMS_PATH)\n",
    "        \n",
    "    \n",
    "\n",
    "    json_data = {}\n",
    "\n",
    "    # Fetch the data for each region and raw material id\n",
    "    print(\"Attempting to fetch and insert data...\")\n",
    "    with alive_bar(len(REGIONS), force_tty=True) as bar:\n",
    "        for r in REGIONS:\n",
    "            \n",
    "            # Fetching the data\n",
    "            data = fetch_data(region_id=r, item_id=item_ids)\n",
    "            \n",
    "            # Storing the data into a postgresql table\n",
    "            insert_data(table_name='market_data', region_id=r, json_data=data)\n",
    "            bar()\n",
    "    \n",
    "    print(\"Successfully stored data for all regions!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/data/items.txt does not exist.\n",
      "Attempting to fetch and insert data...\n",
      "on 0: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 1: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 2: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 3: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 4: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 5: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 6: can only concatenate str (not \"int\") to str                                                                       \n",
      "on 7: can only concatenate str (not \"int\") to str                                                                       \n",
      "|████████████████████████████████████████| 8/8 [100%] in 3.8s (2.10/s)                                                  \n",
      "Successfully stored data for all regions!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fetch_and_store_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c7c545e18f71168b0f0faa889157618686ac54bcfd377c91147b26dbd4ffa70"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
